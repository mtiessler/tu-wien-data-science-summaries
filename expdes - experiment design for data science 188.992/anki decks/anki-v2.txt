#separator:tab
#html:true
#guid column:1
#notetype column:2
#deck column:3
#tags column:6
M`2@]W;O?%	KaTeX and Markdown Basic	expdes exam	"Name the 7 ""ACM Statements on Algorithmic Transparency and Accountability"""	1. awareness<br>2. access and redress<br>3. accountability<br>4. explanation<br>5. data provenance<br>6. auditability<br>7. validation and testing	
tZ2fB~n4)C	KaTeX and Markdown Basic	expdes exam	"For each of the 7 ""ACM Statements on Algorithmic Transparency and Accountability"":<br><br>1. awareness<br>2. access and redress<br>3. accountability<br>4. explanation<br>5. data provenance<br>6. auditability<br>7.&nbsp;validation and testing<br><br>explain:<br><br>- (a) what problem they address<br>- (b) what can be done to solve this problem<br>"	1. awareness<br>&nbsp; &nbsp; - problem - harm and bias in design/implementation/use to individuals/society<br>&nbsp; &nbsp; - solution - stakeholders should be aware<br>2. access and redress<br>&nbsp; &nbsp; - problem - those harmed don't get help<br>&nbsp;&nbsp;&nbsp; - solution - regulators should enforce mechanisms for questioning and redress of grievances<br>3. accountability<br>&nbsp; &nbsp; - problem - institutions deflect responsibility<br>&nbsp;&nbsp;&nbsp; - solution - institutions should be held accountable, regardless of how interpretable their models are<br>4. explanation<br>&nbsp; &nbsp; - problem - no interpretability<br>&nbsp; &nbsp; - solution - systems should have interpretability of processes and decisions (especially for public policy)<br>5. data provenance<br>&nbsp; &nbsp; - problem - mined data might have errors or harmful biases<br>&nbsp;&nbsp;&nbsp; - solution - data collection, processing and possible biases should be documented and open (or just accessible to auditors, in case of privacy concerns)<br>6. auditability<br>&nbsp; &nbsp; - problem - harmful cases not logged<br>&nbsp; &nbsp; - solution - models, data, decisions should be logged to audit cases where harm is suspected<br>7. validation and testing<br>&nbsp; &nbsp; - problem - no rigorous validation<br>&nbsp;&nbsp;&nbsp; - solution - models should be regularly validated, results should be documented and open<br>	
r3?j{xC8}x	KaTeX and Markdown Basic	expdes exam	Most prominent *advantages* of automated algorithms making decision, and why?	- scalability<br>- efficiency<br>- consistency<br>- less human bias<br>- more standardized	
d>_wBT<nXy	KaTeX and Markdown Basic	expdes exam	Most prominent *disadvantages* of automated algorithms making decision, and why? (6 types of concerns)	algorithm ethics: 6 types of concerns<br><br>- epistemic = evidence quality issues<br>&nbsp; &nbsp; - i) inconclusive evidence – not reliable<br>&nbsp;&nbsp;&nbsp; - ii) inscrutable evidence – not interpretable<br>&nbsp;&nbsp;&nbsp; - iii) misguided evidence – data has low quality: garbage in, garbage out<br>- normative = fairness concerns<br>&nbsp; &nbsp; - iv) unfair outcomes – discriminating<br>&nbsp;&nbsp;&nbsp; - v) transformative effects – changes our perception and society (ie. profiling)<br>- traceability = accountability problem<br>&nbsp; &nbsp; - vi) traceability – harm can't be traced	
dG^ua$*ybp	KaTeX and Markdown Basic	expdes exam	What are *challenges* in making decisions of algorithms transparent?	- technical / theoretical complexity<br>- inherent opaqueness of black box deep learning models<br>- additional expenses	
Jvg$qGoJ}E	KaTeX and Markdown Basic	expdes exam	What are *disadvantages* of making algorithms decisions transparent?	- security risks = exposing vulns<br>- gaming the system = exposing unwanted mechanics<br>- competitive concerns = theft of intellectual property<br>- reduced performance = simpler models might underfit<br>- cost = interpretability/transparency needs additional labor and expertise	
Gq7QZ9{0R~	KaTeX and Markdown Basic	expdes exam	Given an ad-blocker what performance metric is most relevant + formula.	most relevant metric: precision<br><br>- users are more tolerant of occasionally seeing ads than having broken website features<br>- therefore precision (not blocking legitimate content) is more important than recall (blocking all ads)<br>- alternatively we can optimize for both values using a tuned f1 score<br><br>formula:<br><br>- sensitivity (recall) = tp / (tp + fn) → true positive ratio (how many relevant are selected?)<br>- precision = tp / (tp + fp) → how many selected are relevant?<br>- f1 score = 2 · (prec · rec) / (prec + rec) → punish low values of either, can be tuned	
c{a_yK]H~T	KaTeX and Markdown Basic	expdes exam	What statistical tests can be used to measure performance against another system -&nbsp;assuming dependence of samples?	- i) paired sample t-test<br>&nbsp; &nbsp; - = tests if difference in means from related samples is significantly different (ie. same model under different conditions, two models on the same samples, naturally correlated observations)<br>&nbsp;&nbsp;&nbsp; - most powerful when normality assumptions are met<br>&nbsp;&nbsp;&nbsp; - appropriate since measurements are paired (same test sets)<br>- ii) wilcoxon signed-rank test<br>&nbsp;&nbsp;&nbsp; - = tests whether the median of the differences differs from zero from related samples (alternative to paired sample t-test)<br>&nbsp;&nbsp;&nbsp; - non-parametric alternative to paired t-test, doesn't assume normality<br>&nbsp;&nbsp;&nbsp; - considers magnitude of differences, not just direction<br>&nbsp;&nbsp;&nbsp; - more robust against outliers<br>&nbsp; &nbsp; - useful if precision differences are skewed	
g1c/B]Lw&_	KaTeX and Markdown Basic	expdes exam	What statistical tests can be used to measure performance against another system -&nbsp;assuming independence of samples?	- i) independent/two sample t-test<br>&nbsp;&nbsp;&nbsp; - = tests if difference in means from independent samples is significantly different (ie. perf between two models, baseline vs. model)<br>&nbsp;&nbsp;&nbsp; - assuming normality<br>&nbsp;&nbsp;&nbsp; - larger sample sizes needed compared to dependent tests<br>- ii) mann-whitney u test<br>&nbsp;&nbsp;&nbsp; - = tests the probability that a randomly selected value from one population exceeds a randomly selected value from another population &amp; tests whether the medians of two populations are equal from independent samples (alternative to independent t-tests)<br>&nbsp;&nbsp;&nbsp; - better for small sample sizes<br>&nbsp;&nbsp;&nbsp; - compares medians rather than means, making it more robust	
"q.+%S3:?#+"	KaTeX and Markdown Basic	expdes exam	What statistical tests can be used to measure performance against another system -&nbsp;assuming overlap of training set?	- i) mcnemar's test → doesn't work with continuous variable as metric<br>&nbsp;&nbsp;&nbsp; - = tests whether there are differences in proportions from&nbsp;related samples with binary classification&nbsp;(alternative to paired sample t-test for nominal data - ie. cross validation)<br>&nbsp;&nbsp;&nbsp; - cheap to compute: requires only one fit for each algorithm<br>&nbsp;&nbsp;&nbsp; - particularly suitable for binary classification<br>&nbsp;&nbsp;&nbsp; - low false positive rate and computationally efficient<br>- ii)&nbsp; 5x2cv paired t-test<br>&nbsp;&nbsp;&nbsp; - can handle overlapping training sets in cross-validation: uses 5 iterations of 2-fold cross-validation to avoid excessive training set overlap<br>&nbsp;&nbsp;&nbsp; - the combined 5x2cv F-test is recommended over the t-test version<br>- iii) friedman test → compares multiple folds at once<br>&nbsp;&nbsp;&nbsp; - = tests whether samples come from the same distribution by comparing ranks from 3 or more related samples (alternative to one-way / repeated-measures anova - ie. repeated measurements on same subjects)<br>&nbsp;&nbsp;&nbsp; - suitable for comparing 3+ algorithms across CV folds, accounts for dependencies<br>&nbsp;&nbsp;&nbsp; - uses rank orders rather than raw values, making it robust to the dependency issues<br>&nbsp;&nbsp;&nbsp; - can be followed by post-hoc tests with appropriate corrections (like Bonferroni) to determine specific differences between models	
oU/Aw&B+nA	KaTeX and Markdown Basic	expdes exam	What errors can be made in statistical testing and how can they be reduced?	$\alpha$ and $\beta$ are inversely related&nbsp;- reducing one usually increases the other<br><br>$\alpha$ = false positive / wrongly reject $H_0$<br><br>- lower significance level $\alpha$<br>- increase sample size:<br>&nbsp;&nbsp;&nbsp; - larger samples narrow confidence intervals / reduces std err. but if we already have enough confidence, it just inflates any effect to significance<br>- optimize number of expriments - or do bonferroni correction:<br>&nbsp;&nbsp;&nbsp; - repeated/parallel testing inflates probability of false positives (multiplicity problem)<br>&nbsp;&nbsp;&nbsp; - for independent tests, the probability compounds:&nbsp;$\bar{\alpha} = 1 - (1 - \alpha)^{m}$ (= family-wise error rate)<br>&nbsp;&nbsp;&nbsp; - for 5 tests the significance level becomes: $\bar \alpha = 1 - (1 - 0.05)^{5} = 0.23$<br>&nbsp;&nbsp;&nbsp; - bonferroni correction = divide $\alpha$ by the $m$ number of tests performed ($\alpha/m$) → makes significant results harder to obtain<br>- use correct tests for train data overlap / dependent samples:<br>&nbsp;&nbsp;&nbsp; - in cross-validation there is a training set overlap (ie. in 5-fold-cv around 75% of train sets are shared for pairwise fold comparisons)<br>&nbsp;&nbsp;&nbsp; - even when data is normally distributed, the overlapping training sets create dependencies between folds that invalidate the use of standard paired tests<br>&nbsp;&nbsp;&nbsp; - use non parametrized or specialized tests instead: McNemar test, 5x2 cross-validation paired t-test<br><br>$\beta$ = false negative / wrongly accept $H_0$<br><br>$1 \text{–} \beta$ = power of test / sensitivity / true positive rate (the test's ability to detect a difference, when one exists)<br><br>- increase sample size<br>- prefer tests with higher power<br>&nbsp; &nbsp; - usually parametric tests are better than non-parametric ones, due to knowing distribution params<br>- for fixed $\alpha$: reduce $\delta$<br>&nbsp; &nbsp; - the smaller the effect (difference between groups $\delta \text{=} |\mu_0 \text{–} \mu_1|$), the harder it becomes to tell the two hypotheses apart, which increases the probability for a false negative<br>- for fixed $\delta$: increase $\alpha$	
Fr/Txw920F	KaTeX and Markdown Basic	expdes exam	Given a dataset of social media texts with timestamps, possible evaluation strategies could be *time-based split* or *cross-validation*.<br><br>- What are advantages and disadvantages of the two options<br>- Which one would you choose and why	time-based split:<br><br>- variations: forward-chaining-cv, block-chaining, rolling window, expanding window, …<br>- important for: time series forecasting, behavioral modeling, customer prediction models<br>- pros:<br>&nbsp;&nbsp;&nbsp; - considers temporal dependencies&nbsp;→ avoids data leakage, probably very predictive<br>&nbsp;&nbsp;&nbsp; - better at handling concept drift →&nbsp;user data distribution might drift with time (ie. user behavior and language evolve over time)<br>- cons:<br>&nbsp;&nbsp;&nbsp; - sensitive to data sparsity, not suitable for small datasets<br>&nbsp;&nbsp;&nbsp; - sensitive to temporal anomalies<br><br>cross-validation:<br><br>- i) shuffle<br>- ii) split data in $k$ parts (folds)<br>- iii) use $\frac{1}{k}$ testing, $\frac{k-1}{k}$ for training<br>- iv) evaluate $k$ times, each time using a different fold<br>- pros:<br>&nbsp;&nbsp;&nbsp; - less sensitive ot data distribution anomalies<br>&nbsp;&nbsp;&nbsp; - lets you use more of your data for both training and eval<br>- cons:<br>&nbsp;&nbsp;&nbsp; - more compute intensive than a single split<br>&nbsp;&nbsp;&nbsp; - can't use temporal dimension in dataset effectively<br><br>probably time based split makes more sense.	
G~.>D>?.|x	KaTeX and Markdown Basic	expdes exam	Name all (discrete) metrics and their formulas	- accuracy = (tp + tn) / (tp + fp + tn + fn)<br>- specificity = tn / (tn + fp)<br>&nbsp; &nbsp; - true negative ratio<br>- sensitivity/recall = tp / (tp + fn)<br>&nbsp; &nbsp; - true positive ratio<br>&nbsp; &nbsp; - how many relevant are selected?<br>- precision = tp / (tp + fp)<br>&nbsp; &nbsp; - how many selected are relevant?<br>- f1 score = 2 · (prec · rec) / (prec + rec)<br>&nbsp; &nbsp; - punish low values of either, can be tuned<br><br>should be computed for each class, as binary: class vs all others	
I!o;YmDv[Z	KaTeX and Markdown Basic	expdes exam	Given a cost matric, where false positives cost 2 units and false negatives cost 12 units,&nbsp;would you optimize the&nbsp;machine learning classifier towards Precision or Recall? Explain your answer!	- the cost of false negatives (12) is a lot higher than false positives (2)<br>- we should optimize for recall rather than precision to minimize the total cost of misclassification → recall minimizes false negatives, by measuring the proportion of actual positives correctly identified<br>- while this might lead to more false positives, their lower cost makes this trade-off acceptable<br><br>formula:<br><br>- sensitivity/recall = tp / (tp + fn)<br>- precision = tp / (tp + fp)	
hC{YPDaO@:	KaTeX and Markdown Basic	expdes exam	For comparison of two machine learning based&nbsp;email spam classifiers&nbsp;A and B, a ground truth annotated dataset is randomly split into 90% training data and 10% test data. Training and test data are used in the same manner for both A and B. This procedure is carried out 20 times.<br><br>Explain the concept of *repeated test-train spilts*.<br><br>Explain how it differs from other evaluation setups.<br><br>What are advantages and disadvantages?	- (repeated) holdout split:<br>&nbsp;&nbsp;&nbsp; - shuffle, split into 2 sets (test, train), repeat<br>&nbsp;&nbsp;&nbsp; - pros:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - better understanding of model stability and performance variance → dependency on a single random split might be unrepresentative<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - allows estimation of confidence intervals for performance metrics<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - works well with small samples, where a single split may not be reliable<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - more flexible than k-fold CV since number of repetitions and split ratios can be freely chosen<br>&nbsp;&nbsp;&nbsp; - cons:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - bias: samples can be under/over-represented in training sets<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - sensitive to random seed selection<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - more compute intensive than a single split<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - may not be necessary for very large datasets where a single split could be sufficient<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - not suitable for time-series data<br>- bootstrapping:<br>&nbsp;&nbsp;&nbsp; - i) shuffle<br>&nbsp;&nbsp;&nbsp; - ii) sample $n$ data points with replacement, meaning each observation has an equal probability of being selected each time<br>&nbsp;&nbsp;&nbsp; - iii) repeat $k$ times, generating $k$ datasets<br>&nbsp;&nbsp;&nbsp; - pros:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - robust error estimates<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - works well with small datasets<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - allows for estimation of statistical properties<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - good for ensemble methods like bagging<br>&nbsp;&nbsp;&nbsp; - cons:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - bias: sample might be overrepresented in train set → errors due to duplicate samples<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - samples can be overrepresented in training set<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - computationally most intensive<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - may overestimate model performance<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - can be sensitive to outliers<br>- cross validation:<br>&nbsp;&nbsp;&nbsp; - i) shuffle<br>&nbsp;&nbsp;&nbsp; - ii) split data in $k$ parts (folds)<br>&nbsp;&nbsp;&nbsp; - iii) use $\frac{1}{k}$ testing, $\frac{k-1}{k}$ for training<br>&nbsp;&nbsp;&nbsp; - iv) evaluate $k$ times, each time using a different fold<br>&nbsp;&nbsp;&nbsp; - leave-1-out-cv = $k$ only has 1 element, accurate but too compute intensive<br>&nbsp;&nbsp;&nbsp; - stratified-cv = each fold must have the same class distribution as the original dataset<br>&nbsp;&nbsp;&nbsp; - nested-cv = also cv within the train set to find hyperparams<br>&nbsp;&nbsp;&nbsp; - forward-chaining-cv = successively adds more historical timeseries data for the same test set<br>&nbsp;&nbsp;&nbsp; - pros:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - bias: samples occur exactly once over all train sets<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - more stable estimates<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - typically less compute intensive (ie. k=10) than repeated holdout splits<br>&nbsp;&nbsp;&nbsp; - cons:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - may not preserve data distribution in each fold<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - requires careful stratification for imbalanced datasets<br><br>best practices for the spam classifier comparison:<br><br>- multiplicity problem / bonferroni correction:<br>&nbsp;&nbsp;&nbsp; - the probability of obtaining at least one false positive increases with each additional test<br>&nbsp;&nbsp;&nbsp; - bonferroni corrected significance level:&nbsp;0.05/20=0.0025<br>&nbsp;&nbsp;&nbsp; - makes it harder to detect genuine differences between the classifiers<br>- average performance metrics across all 20 iterations to get stable estimates<br>- use the same random splits (and seeds) for both classifiers A and B to ensure fair comparison<br>- consider stratified sampling to maintain class distribution across splits if spam/non-spam ratio is imbalanced	
jTe/mf;Mc)	KaTeX and Markdown Basic	expdes exam	For comparison of two machine learning based&nbsp;email spam classifiers&nbsp;A and B, a ground truth annotated dataset is randomly split into 90% training data and 10% test data. Training and test data are used in the same manner for both A and B. This procedure is carried out 20 times.<br><br>Which evaluation metric would you choose?	chosen metric: precision<br><br>users are more tolerant of occasionally seeing ads than missing important emails, therefore precision (not blocking legitimate content) is more important than recall (blocking all ads)	
P+fr1<}HvA	KaTeX and Markdown Basic	expdes exam	Interpret these statements, find an example for each, and how to implement these recommendations.<br><br>- A. Though I will use models boldly to estimate value, I will not be overly impressed by mathematics.<br>- B. I will remember that I didn't make the world, and it doesn't satisfy my equations.<br>- C. Acknowledge that data are people and can do harm.<br>- D. Consider the strengths and limitations of your data; big does not automatically mean better<br>- E. Recognize that privacy is more than a binary value.<br>- F. Design your data and systems for auditability.	"- A) Though I will use models boldly to estimate value, I will not be overly impressed by mathematics.<br>- B) I will remember that I didn't make the world, and it doesn't satisfy my equations.<br><br>- = don't place excessive faith in models. essentially, ""all models are wrong, but some are useful"". no model can fully capture real-world complexity.<br><br>&nbsp;&nbsp;&nbsp; - examples: financial modeling. during the 2008 financial crisis sophisticated risk models failed to predict market collapse. black swan events can happen anytime.<br>&nbsp;&nbsp;&nbsp; - measures: document implicit model assumptions and limitations, test models against real-world scenarios, be skeptic of model decisions, update model on concept drift, don't oversimplify reality.<br><br>- C) Acknowledge that data are people and can do harm.<br><br>- = careless handling of data can cause tangible harm to people.<br><br>&nbsp; &nbsp; - examples: data leaks can make sensitive information public.<br>&nbsp;&nbsp;&nbsp; - measures: treat all data as potentially sensitive until proven otherwise, think of ethics, protect privacy.<br><br>- D) Consider the strengths and limitations of your data; big does not automatically mean better<br><br>- = emphasizes the importance of data quality and context, limitations, biases, potential conflicts of interest.<br><br>&nbsp;&nbsp;&nbsp; - examples: medical records might exclude certain demographic groups due to limited healthcare access, leading to biased conclusions.<br>&nbsp;&nbsp;&nbsp; - measures: document implicit data limitations, biases, provenance, evolution, context in which data was collected. consider whether the data is appropriate for the intended analysis.<br><br>- E) Recognize that privacy is more than a binary value.<br><br>- = privacy exists on a spectrum and depends on context, data type, potential uses rather than being simply ""private"" or ""public"".<br><br>&nbsp; &nbsp; - examples: health data might be considered private in most contexts, but sharing it with medical researchers.<br>&nbsp;&nbsp;&nbsp; - measures: consider varying levels of data sensitivity, adapt privacy measures to specific use cases, implement contextual privacy protections.<br><br>- F) Design your data and systems for auditability.<br><br>- = create transparent/interpretable/tracable systems that can be reviewed and evaluated for accuracy, fairness, ethical compliance and ensure accountability.<br>&nbsp;&nbsp;&nbsp; - examples: high-stakes decision-making systems like loan approval algorithms, auditability allows for detecting and correcting biases or errors that might discriminate against certain groups.<br>&nbsp;&nbsp;&nbsp; - measures: implement logging systems for model decisions and performance, create clear audit/documentation trails for decisions, design transparent processes for external review."	
"kh1#3l^Z`^"	KaTeX and Markdown Basic	expdes exam	Data Management Plans:<br><br>Explain the five common themes / key aspects that need to be addressed in a data management plan and provide examples for each.	- checklist template, tailored to community, for awareness training<br>- living document, changes through project<br>- should follow fair-principles<br>- i) data set description<br>&nbsp;&nbsp;&nbsp; - aka. description of data to be collected &amp; created<br>&nbsp;&nbsp;&nbsp; - = type, source, volume, format<br>- ii) standards, metadata<br>&nbsp;&nbsp;&nbsp; - aka. methodologies for data collection &amp; management<br>&nbsp;&nbsp;&nbsp; - = methodology, community standards, experiment setup details (who, when, conditions, tools, versions), metadata formats (ie. dublin core, premis)<br>- iii) ethics, intellectual property<br>&nbsp;&nbsp;&nbsp; - = privacy, sensitive data, rights, permissions, consent<br>- iv) data sharing<br>&nbsp;&nbsp;&nbsp; - aka. plans for data sharing and acces<br>&nbsp;&nbsp;&nbsp; - = version, access privileges, embargo periods, storage location, mutability, license<br>&nbsp;&nbsp;&nbsp; - open data sharing = lets you cite, license, search data (ie.&nbsp; zenedo, re3data, data repo)<br>- vi) archiving, preservation<br>&nbsp;&nbsp;&nbsp; - aka. strategy for long-term preservation<br>&nbsp;&nbsp;&nbsp; - = lifetime, which data to store, duration, storage/backup strategy, cost considerations, repository choice<br>&nbsp;&nbsp;&nbsp; - persistent identifiers = user ids (orcid), data object identifier (doi) → physical location can change	
A{qy9N2Nmi	KaTeX and Markdown Basic	expdes exam	Data Management Plans:<br><br>Describe the motivation and key concepts underlying machine-actionable DMPs	"- advantages of madmps: (based on semantic web)<br>&nbsp; &nbsp; - not just for awareness training, compliance<br>&nbsp; &nbsp; - not maintained by humans, less error prone, dynamic<br>&nbsp; &nbsp; - uses community standards, vocabularies (ie. dublin core, premis)<br>&nbsp;&nbsp;&nbsp; - uses persistent ids like orcid or doi<br>&nbsp;&nbsp;&nbsp; - structured, machine-readable/actionable/verifiable format, not just ""promises"", complex queries by stakeholders"	
fHx}Q@3bt?	KaTeX and Markdown Basic	expdes exam	List and describe common sources of irreproducibility from a computing perspective	- technical sources<br>&nbsp;&nbsp;&nbsp; - software dependencies: undefined dependencies/versions, missing/incomplete/malfunctioning code, undocumented params and configs, breaking apis<br>&nbsp;&nbsp;&nbsp; - hardware dependencies: non-portability, unknown arch and os, build issues, filesystem/encoding differences<br>- data related sources<br>&nbsp;&nbsp;&nbsp; - missing/inaccessible data, undefined versions/subset indices/preprocessing steps, difference in quality/format, data drift over time<br>- documentation sources:<br>&nbsp;&nbsp;&nbsp; - missing documentation/metadata/provenance information on system/process/experimental design - especially for manual<br>- process related sources<br>&nbsp;&nbsp;&nbsp; - experimental setup: non-deterministic behavior, unspecified rng seed, timing and performance variation, order-dependent operations with side effects, missing environment variables<br>&nbsp;&nbsp;&nbsp; - human factors: complexity, implicit knowledge, subjectivity/lack of standardization	
EBW?~UK9Qh	KaTeX and Markdown Basic	expdes exam	Describe the PRIMAD model of reproducibility types and explain the insights gained by priming (modifying) the respective elements	- some benefits of priming overlap:<br>&nbsp;&nbsp;&nbsp; - any element (except actor, research objective) improves correctness of hypothesis<br>&nbsp;&nbsp;&nbsp; - repeating as an effort achieves determinism through exact replication<br>- primad:<br>&nbsp;&nbsp;&nbsp; - p - platform/stack<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = software env and infrastructure used to conduct experiments<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: porting<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: portability<br>&nbsp;&nbsp;&nbsp; - r - research objective<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = goals and questions being investigated<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: re-using<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: re-purpose, re-using code in different cross-disciplinary settings, resource efficiency, correctness of hypothesis<br>&nbsp;&nbsp;&nbsp; - i - implementation<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = software implementation of methodology<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: re-coding<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: correctness of implementation, portability, efficiency, more outreach<br>&nbsp;&nbsp;&nbsp; - m - method<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = methodology, algorithms<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: validating<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: correctness of hypothesis, validation with different methodogical approaches<br>&nbsp;&nbsp;&nbsp; - a - actors<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = people involved in conducting the experiments<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: independent verification<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: sufficiency of information, independent verification, transparency<br>&nbsp;&nbsp;&nbsp; - d - raw data<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = raw data used in the experiment<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: generalizing<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: re-use for different settings<br>&nbsp;&nbsp;&nbsp; - d - parameters<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = config values/settings that control the experimental process<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: parameter sweep<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: robustness testing, sensitivity analysis, parameter sweep, determinism	
j*b]{P|.=I	KaTeX and Markdown Basic	expdes exam	What is CV?	- i) shuffle<br>- ii) split data in $k$ parts (folds)<br>- iii) use $\frac{1}{k}$ testing, $\frac{k-1}{k}$ for training<br>- iv) evaluate $k$ times, each time using a different fold	
L2R>aQh$y3	KaTeX and Markdown Basic	expdes exam	What is leave-one-out CV? Explain the benefits/drawbacks	- loocv / leave-1-out-cv = cross validation where $k$ only has 1 element<br>- pros:<br>&nbsp;&nbsp;&nbsp; - minimal bias, uses nearly all data points for training (n-1 observations), exhaustive validation method<br>&nbsp;&nbsp;&nbsp; - valuable for small datasets (where data efficiency is crucial)<br>&nbsp;&nbsp;&nbsp; - deterministic results with no randomness in the splitting process, yields the same performance estimate every time it's run on a given dataset<br>- cons:<br>&nbsp;&nbsp;&nbsp; - too compute intensive, requires training n separate models, where n is the number of observations<br>&nbsp;&nbsp;&nbsp; - can produce high variance in performance metrics, test sets of single observations may lead to unstable performance metrics	
b0DGl(I=|3	KaTeX and Markdown Basic	expdes exam	Workflow: loading the data, standardizing, CV, KNN, averaging results, printing results.<br><br>What is wrong with this workflow?	"- k-fold cross-validation isn't stratified to ensure balanced class distributions<br>- data leakage: the standardization step should be performed within each fold, not on the entire dataset upfront<br>- no parameter optimization for the algorithms (the k parameter for KNN)<br>&nbsp;&nbsp;&nbsp; - the algorithm suffers from the ""curse of dimensionality"" with high-dimensional patent data<br>&nbsp;&nbsp;&nbsp; - the choice of k value significantly impacts results and should be optimized<br><br>- simply repeating CV 20 times with the same split may not give reliable performance estimates"	
j@2<LRnn<T	KaTeX and Markdown Basic	expdes exam	What are the two challenges with data citation and list the (unsuccessful) approaches to overcome them.	"- i) dynamic nature of data<br>&nbsp;&nbsp;&nbsp; - = data gets corrected, extended over time<br>&nbsp;&nbsp;&nbsp; - unsuccessful solutions:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - using ""accessed at"" (no change history or version control)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - aggregating changes into larger releases, delaying releaaes<br>&nbsp;&nbsp;&nbsp; - proposed solution: versioning with timestamps<br>- ii) granularity issues<br>&nbsp;&nbsp;&nbsp; - unsuccessful solutions:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - storing subset (doesn't scale)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - citing textually (imprecise)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - listing indices (imprecise)<br>&nbsp;&nbsp;&nbsp; - proposed solution: query-based citations instead of storing static subsets<br>- iii) technology dependencies<br>&nbsp;&nbsp;&nbsp; - unsuccessful solutions:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - basic manual documentation of data versions (error prone, not reproducible)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - simple doi, static snapshots of databases<br>&nbsp;&nbsp;&nbsp; - proposed solution: technology-independent architecture<br>"	
ehby[z5^{]	KaTeX and Markdown Basic	expdes exam	Describe the approach of the RDA Data Citation WG to resolve&nbsp;challenges with data citation.	recommendations by research data alliance working group on dynamic data citation (rda wg).<br><br>4 recommendations to address 3 challenges.<br><br>- c1) dynamic nature of data (additions/corrections over time)<br>&nbsp;&nbsp;&nbsp; - i) preparing data &amp; query store = data versioning, timestamping, query store → version, timestamp, checksum for each subset<br>- c2) granularity issues (citing precise subsets)<br>&nbsp;&nbsp;&nbsp; - ii) data persistence = query uniqueness, stable sorting, result set verification, query timestamping, query pid, store query, citation text<br>- c3) technology dependencies (stability across system changes)<br>&nbsp;&nbsp;&nbsp; - iii) resolving a pid = landing page (for humans and machines), machine actionability<br>&nbsp;&nbsp;&nbsp; - iv) data infra modifications = technology migration, migration verification → persistent identifier (pid) stays the same even after migration	
cM}(^ZY]a]	KaTeX and Markdown Basic	expdes exam	Given an experimental setup with 10.000 instances, 20-fold cross validation (k-fold with k=20) and accuracy as performance measurement.<br><br>What's the sample size, e.g. what's $N$ in this case.	- there are 20 folds, each fold consisting of 10.000 / 20 = 500 samples<br>- the test set is always 1 fold, therefore $N$ = 500	
1uLe>A(U+	KaTeX and Markdown Basic	expdes summary	data science process (5 steps)	- i) ask question<br>&nbsp;&nbsp;&nbsp; - research question<br>- ii) get data<br>&nbsp;&nbsp;&nbsp; - privacy<br>- iii) explore data<br>&nbsp;&nbsp;&nbsp; - preprocess, clean, transform<br>&nbsp;&nbsp;&nbsp; - missing data, outliers, anomalies, patterns<br>&nbsp;&nbsp;&nbsp; - descriptive statistics, plots<br>- iv) model data<br>&nbsp;&nbsp;&nbsp; - fit, validate<br>&nbsp;&nbsp;&nbsp; - high bias = underfitting, inaccurate<br>&nbsp;&nbsp;&nbsp; - high variance = overfitting, not generalizing<br>- v) communicate results<br>&nbsp;&nbsp;&nbsp; - explain<br>&nbsp;&nbsp;&nbsp; - correlation vs. causation	
s^4yx4WgZb	KaTeX and Markdown Basic	expdes summary	crisp-dm process (6 steps)	- = cross industry standardprocess for data dining<br>- i) business understanding<br>- ii) data understanding<br>- iii) data preparation<br>- iv) modeling<br>- v) evaluation<br>- vi) deployment	
JmqHnm*az>	KaTeX and Markdown Basic	expdes summary	acts and regulations in the european union (1 regulation, 3 acts)	- general data protection regulation (gdpr):<br>&nbsp;&nbsp;&nbsp; - personal data = identity, certificates, documents, records, health, …<br>&nbsp;&nbsp;&nbsp; - citizens control how personal data is processed<br>&nbsp;&nbsp;&nbsp; - no discriminatory algorithms<br>- eu data governance act:<br>&nbsp;&nbsp;&nbsp; - public sector data to be reused for research (ie. in health) but not openly available<br>&nbsp;&nbsp;&nbsp; - common european data spaces - sharing data across borders, sectors<br>&nbsp;&nbsp;&nbsp; - voluntary data contribution by businesses<br>- eu data act:<br>&nbsp;&nbsp;&nbsp; - access to smart device data<br>&nbsp;&nbsp;&nbsp; - fairer contracts for small businesses<br>&nbsp;&nbsp;&nbsp; - data use in emergencies by public authorities<br>&nbsp;&nbsp;&nbsp; - easy cloud service switching by customers<br>- eu ai act (risk based):<br>&nbsp;&nbsp;&nbsp; - minimal risk = no regulation → for common stuff (recommender systems, spam filters, …)<br>&nbsp;&nbsp;&nbsp; - transparency risk = transparency requirements → for risk of impersonation, deception (chatbots, generative models, …)<br>&nbsp;&nbsp;&nbsp; - high risk = compliance assessment in entire life cycle, citizens can file complaints → for a risk to health, safety (vehicles, medical devices, critical infra, general purpose ai…)<br>&nbsp;&nbsp;&nbsp; - unacceptable risk = prohibition → for violation of human rights (social scoring by government, …)	
Ps:8BwD`9Y	KaTeX and Markdown Basic	expdes summary	4 stages of eu ai act (risk based)	- eu ai act (risk based):<br>&nbsp;&nbsp;&nbsp; - minimal risk = no regulation → for common stuff (recommender systems, spam filters, …)<br>&nbsp;&nbsp;&nbsp; - transparency risk = transparency requirements → for risk of impersonation, deception (chatbots, generative models, …)<br>&nbsp;&nbsp;&nbsp; - high risk = compliance assessment in entire life cycle, citizens can file complaints → for a risk to health, safety (vehicles, medical devices, critical infra, general purpose ai…)<br>&nbsp;&nbsp;&nbsp; - unacceptable risk = prohibition → for violation of human rights (social scoring by government, …)	
A-]B)m.xq*	KaTeX and Markdown Basic	expdes summary	algorithm ethics: 6 types of concerns	- epistemic = evidence quality issues<br>&nbsp;&nbsp;&nbsp; - i) inconclusive evidence – not reliable<br>&nbsp;&nbsp;&nbsp; - ii) inscrutable evidence – not interpretable<br>&nbsp;&nbsp;&nbsp; - iii) misguided evidence – data has low quality: garbage in, garbage out<br>- normative = fairness concerns<br>&nbsp;&nbsp;&nbsp; - iv) unfair outcomes – discriminating<br>&nbsp;&nbsp;&nbsp; - v) transformative effects – changes our perception and society (ie. profiling)<br>- traceability – accountability problem<br>&nbsp;&nbsp;&nbsp; - vi) traceability – harm can't be traced	
[k[lDzag[	KaTeX and Markdown Basic	expdes summary	4 experiment types	- pilot experiment = checking instruments<br>- natural experiment = uncontrolled conditions, just observations (ie. economics, meteorology)<br>- field experiment = partially controlled conditions, some stimuli provided (ie. social sciences)<br>- controlled experiment = fully controlled conditions<br><br>good to have:<br><br>- reliable = consistent under repetition, similar conditions<br>- hypothesis = expected effect of input on output	
xyTYo1][%`	KaTeX and Markdown Basic	expdes summary	variable types (6 types)	- input = independent var<br>&nbsp;&nbsp;&nbsp; - values are called 'control'<br>&nbsp;&nbsp;&nbsp; - 'factorial' experiments search input space exhaustively<br>- output = dependent var, returns performance metric<br>- control = kept constant<br>- interfering = extraneous, nuisance, influencing some vars → placebo group tries to study them<br>- confounding = influences both input and output (ie. gender influences selected treatment, chance of recovery)<br>- latent = not directly measurable (ie. user trust)	
J1V*hT0GAS	KaTeX and Markdown Basic	expdes summary	4 scales	<br>- nominal = enums (ie. gender)<br>- ordinal = have order (ie. ratings)<br>- interval = have numeric scale, no zero point / reference point (ie. time)<br>- ratio = have zero point (ie. allow mult, division)	
NKl2J>;-|[	KaTeX and Markdown Basic	expdes summary	3 model validation techniques	- holdout split:<br>&nbsp;&nbsp;&nbsp; - test set, train set, val set (subset of train set or a seperate set)<br>&nbsp;&nbsp;&nbsp; - if you have over 1mio samples, use 98/2 split, otherwise 70/30<br>&nbsp;&nbsp;&nbsp; - bias: sample might be underrepresented in train set<br>- bootstrapping:<br>&nbsp;&nbsp;&nbsp; - i) shuffle<br>&nbsp;&nbsp;&nbsp; - ii) sample $n$ data points with replacement, meaning each observation has an equal probability of being selected each time<br>&nbsp;&nbsp;&nbsp; - iii) repeat $k$ times, generating $k$ datasets<br>&nbsp;&nbsp;&nbsp; - datasets can be used for hyperparam testing<br>&nbsp;&nbsp;&nbsp; - metrics can be aggregated<br>&nbsp;&nbsp;&nbsp; - bias: sample might be overrepresented in train set<br>- cross validation:<br>&nbsp;&nbsp;&nbsp; - i) shuffle<br>&nbsp;&nbsp;&nbsp; - ii) split data in $k$ parts (folds)<br>&nbsp;&nbsp;&nbsp; - iii) use $\frac{1}{k}$ testing, $\frac{k-1}{k}$ for training<br>&nbsp;&nbsp;&nbsp; - iv) evaluate $k$ times, each time using a different fold<br>&nbsp;&nbsp;&nbsp; - leave-1-out-cv = $k$ only has 1 element, accurate but too compute intensive<br>&nbsp;&nbsp;&nbsp; - stratified-cv = each fold must have the same class distribution as the original dataset<br>&nbsp;&nbsp;&nbsp; - nested-cv = also cv within the train set to find hyperparams<br>&nbsp;&nbsp;&nbsp; - forward-chaining-cv = successively adds more historical timeseries data for the same test set<br>&nbsp;&nbsp;&nbsp; - avoid data leak: hyperparam search within the same fold<br>&nbsp;&nbsp;&nbsp; - bias: samples occur exactly once over all train sets	
bU(Wa8[ZwW	KaTeX and Markdown Basic	expdes summary	4 types of cross validation	- leave-1-out-cv = $k$ only has 1 element, accurate but too compute intensive<br>- stratified-cv = each fold must have the same class distribution as the original dataset<br>- nested-cv = also cv within the train set to find hyperparams<br>- forward-chaining-cv = successively adds more historical timeseries data for the same test set	
"e4|xp+j@#B"	KaTeX and Markdown Basic	expdes summary	2 numerical metrics	- numerical:<br>&nbsp;&nbsp;&nbsp; - mae - mean absolute error = $\frac{1}{n} \sum{|\hat y_i - y_i|}$<br>&nbsp;&nbsp;&nbsp; - rmse - root mean squared error = $\sqrt{\frac{1}{n} \sum{\hat y_i - y_i}}$ → punish larger values	
cE%(H(7H/;	KaTeX and Markdown Basic	expdes summary	5 discrete metrics	- discrete:<br>&nbsp;&nbsp;&nbsp; - accuracy = (tp + tn) / (tp + fp + tn + fn)<br>&nbsp;&nbsp;&nbsp; - specificity = tn / (tn + fp) → true negative ratio<br>&nbsp;&nbsp;&nbsp; - sensitivity/recall = tp / (tp + fn) → true positive ratio - how many relevant are selected?<br>&nbsp;&nbsp;&nbsp; - precision = tp / (tp + fp) → how many selected are relevant?<br>&nbsp;&nbsp;&nbsp; - f1 score = 2 · (prec · rec) / (prec + rec) → punish low values of either, can be tuned<br><br>should be computed for each class, as binary: class vs all others	
j%|N1mk&%$	KaTeX and Markdown Basic	expdes summary	ways to find data distribution	- empirical sampling = use sample ratios<br>- parametric statistics = assumes normal distribution (ignored in practice), then changes params<br>- nonparametric statistics = no assumptions on distribution or params, rank-based methods	
B%-K1fC4qp	KaTeX and Markdown Basic	expdes summary	reducing errors in statistic tests	- error types are inversely related&nbsp;- reducing one typically increases the other<br>- $\alpha$ = **false positive** / wrongly reject $H_0$<br>&nbsp;&nbsp;&nbsp; - lower significance level $\alpha$<br>&nbsp;&nbsp;&nbsp; - increase sample size:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - larger samples narrow confidence intervals / reduces std err. but if we already have enough confidence, it just inflates any effect to significance<br>&nbsp;&nbsp;&nbsp; - optimize number of expriments:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - repeated/parallel testing inflates probability of false positives (multiplicity problem)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - for independent tests, the probability compounds:&nbsp;$\bar{\alpha} = 1 - (1 - \alpha)^{m}$ (= family-wise error rate)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - for 5 tests the significance level becomes: $\bar \alpha = 1 - (1 - 0.05)^{5} = 0.23$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - bonferroni correction = divide $\alpha$ by the $m$ number of tests performed ($\alpha/m$) → makes significant results harder to obtain<br>&nbsp;&nbsp;&nbsp; - use correct tests in cross validation:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - in cross-validation there is a training set overlap (ie. in 5-fold-cv around 75% of train sets are shared for pairwise fold comparisons)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - even when data is normally distributed, the overlapping training sets create dependencies between folds that invalidate the use of standard paired tests<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - use non parametrized or specialized tests instead: McNemar test, 5x2 cross-validation paired t-test<br>- $\beta$ = **false negative** / wrongly accept $H_0$<br>&nbsp;&nbsp;&nbsp; - increase power of test<br>&nbsp;&nbsp;&nbsp; - increase sample size<br>- $1 \text{–} \beta$ = **power of test** / sensitivity / true positive rate (the test's ability to detect a difference, when one exists)<br>&nbsp;&nbsp;&nbsp; - tests with higher power should be preferred (usually parametric tests are better than non-parametric ones, due to knowing distribution params)<br>&nbsp;&nbsp;&nbsp; - for fixed $\alpha$: the smaller the effect (difference between groups $\delta \text{=} |\mu_0 \text{–} \mu_1|$), the harder it becomes to tell the two hypotheses apart, which increases the probability for a false negative → when $\delta$ decreases, $\beta$ increases and $1 \text{–} \beta$ decreases<br>&nbsp;&nbsp;&nbsp; - for fixed $\delta$: the power of test increases/decreases with $\alpha$	
j60SW!]}~[	KaTeX and Markdown Basic	expdes summary	central limit theorem (CLT)	- = if you repeatedly take samples from any population and average them, those averages will be normally distributed, regardless of the original population's distribution<br>- for $N\geq30$ samples<br>- sample:<br>&nbsp;&nbsp;&nbsp; - $\sigma_{\bar{x}} = \sigma / \sqrt N$ = standard error (std dev of the mean of the means of samples)<br>&nbsp;&nbsp;&nbsp; - $\bar{x} \approx \mu$ as $n \rightarrow \infty$<br>- population:<br>&nbsp;&nbsp;&nbsp; - if $\sigma$ is unknown: make an estimate $\hat \sigma := \sigma_{\bar{x}}$ (or simply $s$)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - this is commonly the case<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - then $\sigma_{\bar{x}} \text{=} \sigma / \sqrt N$ becomes $\hat \sigma_{\bar{x}} \text{=} s / \sqrt N$<br>&nbsp;&nbsp;&nbsp; - if $\mu, \sigma$ is unknown:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - make up an arbitrary mean for the population as the threshold/target value (not recommended)	
iy/!l].P+V	KaTeX and Markdown Basic	expdes summary	all parametrized tests	- parameterized tests:<br>&nbsp;&nbsp;&nbsp; - *z-test* = tests if **new observed mean** $\bar x$ is significantly different from population mean (if $N\geq30$, $\sigma$ known)<br>&nbsp;&nbsp;&nbsp; - *single sample t-test* = tests if **new observed mean** $\bar x$ is significantly different from population mean (if $N&lt;30$, $\sigma$ unknown)<br>&nbsp;&nbsp;&nbsp; - *independent/two sample t-test* = tests if difference in means from **independent samples** is significantly different (ie. perf between two models, baseline vs. model)<br>&nbsp;&nbsp;&nbsp; - *paired sample t-test* = tests if difference in means from **related samples** is significantly different (ie. same model under different conditions, two models on the same samples, naturally correlated observations)<br>&nbsp;&nbsp;&nbsp; - *anova* = tests if difference in means from **3 or more independent samples** is significantly different - you can then identify that one using post-hoc tests (ie. instead of multiple t-tests)	
"pvW$(h#yua"	KaTeX and Markdown Basic	expdes summary	all non-parametrized tests	- non-parameterized tests:<br>&nbsp;&nbsp;&nbsp; - when normality can't be assumed<br>&nbsp;&nbsp;&nbsp; - *sign test* = tests whether the median of the differences differs from zero from **related samples** (alternative to paired sample t-test, less power but simpler)<br>&nbsp;&nbsp;&nbsp; - *wilcoxon signed-rank test* = tests whether the median of the differences differs from zero from **related samples** (alternative to paired sample t-test)<br>&nbsp;&nbsp;&nbsp; - *mann-whitney u test* = tests the probability that a randomly selected value from one population exceeds a randomly selected value from another population &amp; tests whether the medians of two populations are equal from **independent samples** (alternative to independent t-tests)<br>&nbsp;&nbsp;&nbsp; - *kruskal-walllis test* = tests whether samples come from the same distribution by comparing mean ranks from **2 or more independent samples** (alternative to one-way anova, only makes sense from ≥3 groups)<br>&nbsp;&nbsp;&nbsp; - *friedman test* = tests whether samples come from the same distribution by comparing ranks from **3 or more related samples** (alternative to one-way / repeated-measures anova - ie. repeated measurements on same subjects)<br>&nbsp;&nbsp;&nbsp; - *mcnemar test*&nbsp;= tests whether there are differences in proportions from&nbsp;**related samples** with binary classification&nbsp;(alternative to paired sample t-test for nominal data - ie. cross validation)	
E/|&}}3Qa^	KaTeX and Markdown Basic	expdes summary	z-test	- = tests if new observed mean $\bar x$ is significantly different from population mean (if $N\geq30$, $\sigma$ known)<br>- assumes: $N\geq30$ samples, $\sigma$ known, normality, independence, interval/ratio scale level, no outliers<br>- $Z = \frac{\bar{x} - \mu}{{\sigma} / \sqrt{n}}$ → converts to z-distribution / standard distribution where $\mu \text{=} 0, s \text{=} 1$	
bC4s&mux;L	KaTeX and Markdown Basic	expdes summary	single sample t-test	- assumes: $N&lt;30$ samples, $\sigma$ unknown, (normality, independence, interval/ratio scale level, no outliers)<br>- uses std err to approximate sigma, so $\sigma_{\bar{x}} \text{=} \sigma / \sqrt N$ becomes $\hat \sigma_{\bar{x}} \text{=} s / \sqrt N$<br>- t-distribution has heavier tails, because we have less confidence and more extreme values<br><br>- **single sample t-test**:<br>&nbsp;&nbsp;&nbsp; - = tests if new observed mean $\bar x$ is significantly different from population mean (if $N&lt;30$, $\sigma$ unknown)<br>&nbsp;&nbsp;&nbsp; - use t-distribution with $N \text{–} 1$ degrees of freedom<br>&nbsp;&nbsp;&nbsp; - $t = \frac{\bar{x} - \mu}{{s} / \sqrt{n}}$	
BtIj1Y1rXx	KaTeX and Markdown Basic	expdes summary	two sample t-test	- assumes: $N&lt;30$ samples, $\sigma$ unknown, (normality, independence, interval/ratio scale level, no outliers)<br>- uses std err to approximate sigma, so $\sigma_{\bar{x}} \text{=} \sigma / \sqrt N$ becomes $\hat \sigma_{\bar{x}} \text{=} s / \sqrt N$<br>- t-distribution has heavier tails, because we have less confidence and more extreme values<br><br>- **two sample t-test**:<br>&nbsp;&nbsp;&nbsp; - = tests if difference in means from independent samples is significantly different (ie. perf between two models, baseline vs. model)<br>&nbsp;&nbsp;&nbsp; - use t-distribution with $n_1 \text{+} n_2 \text{–} 2$ degrees of freedom (df) for critical values<br>&nbsp;&nbsp;&nbsp; - can be one-tailed or two-tailed test<br>&nbsp;&nbsp;&nbsp; - $t_{\bar{x}_{1}-\bar{x}_{2}} = \frac{\bar{x}_{1} - \bar{x}_{2}}{\hat{\sigma}_{\bar{x}_{1}-\bar{x}_{2}}}$<br>&nbsp;&nbsp;&nbsp; - standard error = $\hat{\sigma}_{\bar{x}_{1}-\bar{x}_{2}} = \sqrt{\hat{\sigma}^{2}_{pooled}(\frac{1}{{N}_{1}} + \frac{1}{{N}_{2}})}$<br>&nbsp;&nbsp;&nbsp; - pooled variance = $\hat{\sigma}^2_{pooled} = \frac{(N_1 - 1)s^2_1 + (N_2 - 1)s^2_2}{N_1 + N_2 - 2}$	
jIZ*+H&,;9	KaTeX and Markdown Basic	expdes summary	paired sample t-test	- assumes: $N&lt;30$ samples, $\sigma$ unknown, (normality, independence, interval/ratio scale level, no outliers)<br>- uses std err to approximate sigma, so $\sigma_{\bar{x}} \text{=} \sigma / \sqrt N$ becomes $\hat \sigma_{\bar{x}} \text{=} s / \sqrt N$<br>- t-distribution has heavier tails, because we have less confidence and more extreme values<br><br>- **paired sample t-test:**<br>&nbsp;&nbsp;&nbsp; - = tests if difference in means from related samples is significantly different (ie. same model under different conditions, two models on the same samples, cross validation, naturally correlated observations)<br>&nbsp;&nbsp;&nbsp; - requires fewer subjects since each person serves as their own control<br>&nbsp;&nbsp;&nbsp; - minimizes variance (by having half as many test problems), increases confidence<br>&nbsp;&nbsp;&nbsp; - $t_\delta = \frac{\bar{x}_{\delta} - {\mu}_{\delta}}{s_{\delta} / \sqrt{N_{\sigma}}}$<br>&nbsp;&nbsp;&nbsp; - i) compute differences $\delta = |\mu_1 - \mu_2|$<br>&nbsp;&nbsp;&nbsp; - ii) mean $\mu_{\delta}$ and std dev $\hat \sigma_{\delta}$ of differences<br>&nbsp;&nbsp;&nbsp; - iii) initially assume that there is no significant difference $H_0 \text{: } \mu_{\delta} = 0$<br>&nbsp;&nbsp;&nbsp; - iv) compute test statistics	
"jo@`b4#p7N"	KaTeX and Markdown Basic	expdes summary	anova	<br>- = tests if difference in means from 3 or more independent samples is significantly different - you can then identify that one using post-hoc tests (ie. instead of multiple t-tests)<br>- assumes: normality, independence, homogenity of variance<br>- $H_0 \text{: } \mu_1 = \mu_2 = \dots = \mu_k$	
ue|pkf2fCR	KaTeX and Markdown Basic	expdes summary	sign-test	- doesn't assume normal distribution in data → but in practice the assumptions for parametric tests are just ignored and stuff still works well<br>- for non-interval/ratio data, ordinal scales, data with outliers<br>- **sign test**:<br>&nbsp;&nbsp;&nbsp; - = tests whether the median of the differences differs from zero from related samples (alternative to paired sample t-test)<br>&nbsp;&nbsp;&nbsp; - assumes: ordinal scales, paired samples<br>&nbsp;&nbsp;&nbsp; - less powerful than other tests, but simpler to use<br>&nbsp;&nbsp;&nbsp; - null hypothesis assumes 50/50 chance of one value being larger than the other<br>&nbsp;&nbsp;&nbsp; - $H_0 \text{: } \text{Pr}(X &gt; Y) = 0.5$	
Ob]I5K}(]$	KaTeX and Markdown Basic	expdes summary	wilcoxon signed-rank test	- doesn't assume normal distribution in data → but in practice the assumptions for parametric tests are just ignored and stuff still works well<br>- for non-interval/ratio data, ordinal scales, data with outliers<br><br>- **wilcoxon signed-rank test**:<br>&nbsp;&nbsp;&nbsp; - = tests whether the median of the differences differs from zero from related samples (alternative to paired sample t-test)<br>&nbsp;&nbsp;&nbsp; - tests whether the distribution of differences is symmetrical around zero<br>&nbsp;&nbsp;&nbsp; - assumes: interval/ordinal scales, paired samples (before/after measurements)<br>&nbsp;&nbsp;&nbsp; - better than sign test because it considers the magnitude of differences	
j4t:.--~r0	KaTeX and Markdown Basic	expdes summary	mann-whitney u test	- doesn't assume normal distribution in data → but in practice the assumptions for parametric tests are just ignored and stuff still works well<br>- for non-interval/ratio data, ordinal scales, data with outliers<br><br>- **mann-whitney u test**:<br>&nbsp;&nbsp;&nbsp; - = tests the probability that a randomly selected value from one population exceeds a randomly selected value from another population &amp; tests whether the medians of two populations are equal from independent samples (alternative to independent t-tests)	
wo3O;m7P8[	KaTeX and Markdown Basic	expdes summary	kruskal-walllis test	- doesn't assume normal distribution in data → but in practice the assumptions for parametric tests are just ignored and stuff still works well<br>- for non-interval/ratio data, ordinal scales, data with outliers<br><br>- **kruskal-walllis test**:<br>&nbsp;&nbsp;&nbsp; - = tests whether samples come from the same distribution by comparing mean ranks from 2 or more independent samples (alternative to one-way anova, only makes sense from ≥3 groups)<br>&nbsp;&nbsp;&nbsp; - works with unequal sample sizes	
k[N3IgB^VO	KaTeX and Markdown Basic	expdes summary	friedman test	- doesn't assume normal distribution in data → but in practice the assumptions for parametric tests are just ignored and stuff still works well<br>- for non-interval/ratio data, ordinal scales, data with outliers<br><br>- **friedman test**:<br>&nbsp;&nbsp;&nbsp; - = tests whether samples come from the same distribution by comparing ranks from 3 or more related samples (alternative to one-way / repeated-measures anova - ie. repeated measurements on same subjects)<br>&nbsp;&nbsp;&nbsp; - good for repeated measurements on same subjects	
t2Yx]!i]M8	KaTeX and Markdown Basic	expdes summary	8 data citation principles	- 8 data citation principles:<br>&nbsp;&nbsp;&nbsp; - importance = data is just as important as the publication<br>&nbsp;&nbsp;&nbsp; - credit and attribution = to authors<br>&nbsp;&nbsp;&nbsp; - evidence = for claims<br>&nbsp;&nbsp;&nbsp; - unique id = for persistence, even when underlying layer changes<br>&nbsp;&nbsp;&nbsp; - access = for human/machine accessibility<br>&nbsp;&nbsp;&nbsp; - persistence = uses unique id<br>&nbsp;&nbsp;&nbsp; - specificity and verifiability = provenance, for access time, version, portion/subset of data<br>&nbsp;&nbsp;&nbsp; - interoperability and flexibility = among different communities<br>- other benefits:<br>&nbsp;&nbsp;&nbsp; - giving/receiving credit, re-using existing knowledge, preventing scientific misconduct<br>&nbsp;&nbsp;&nbsp; - identification, documentation, context, impact, transparency, reproducibility	
i}qXBE@twK	KaTeX and Markdown Basic	expdes summary	6 parts of data management plan content	- checklist template, tailored to community, for awareness training<br>- living document, changes through project<br>- should follow fair-principles<br>- i) data set description<br>&nbsp;&nbsp;&nbsp; - aka. description of data to be collected &amp; created<br>&nbsp;&nbsp;&nbsp; - = type, source, volume, format<br>- ii) standards, metadata<br>&nbsp;&nbsp;&nbsp; - aka. methodologies for data collection &amp; management<br>&nbsp;&nbsp;&nbsp; - = methodology, community standards, experiment setup details (who, when, conditions, tools, versions), metadata formats (ie. dublin core, premis)<br>- iii) ethics and Intellectual property<br>&nbsp;&nbsp;&nbsp; - = privacy, sensitive data, rights, permissions, consent<br>- iv) data sharing<br>&nbsp;&nbsp;&nbsp; - aka. plans for data sharing and access<br>&nbsp;&nbsp;&nbsp; - = version, access privileges, embargo periods, storage location, mutability, license<br>&nbsp;&nbsp;&nbsp; - open data sharing = lets you cite, license, search data (ie.&nbsp; zenedo, re3data, data repo)<br>- vi) archiving, preservation<br>&nbsp;&nbsp;&nbsp; - aka. strategy for long-term preservation<br>&nbsp;&nbsp;&nbsp; - = lifetime, which data to store, duration, storage/backup strategy, cost considerations, repository choice<br>&nbsp;&nbsp;&nbsp; - persistent identifiers = user ids (orcid), data object identifier (doi) → physical location can change	
koSWU:Vz|V	KaTeX and Markdown Basic	expdes summary	fair principles	- f - findable = metadata for search<br>- a - accessible = for machines, clear access privileges<br>- i - interoperable = across different communities and domains<br>- r - reusable = clear license, documentation (sum of the 3 other rules)	
nG%;u^~[RD	KaTeX and Markdown Basic	expdes summary	advantages of a machine actionable dmp (madmp)	"- disadvantages of dmps:<br>&nbsp;&nbsp;&nbsp; - just for awareness training, compliance<br>&nbsp;&nbsp;&nbsp; - tedious to maintain by humans, error prone<br>&nbsp;&nbsp;&nbsp; - can't be verified, machine processed, just ""promises""<br>- advantages of madmps: (based on semantic web)<br>&nbsp;&nbsp;&nbsp; - uses community standards, vocabularies (ie. dublin core, premis)<br>&nbsp;&nbsp;&nbsp; - uses persistent ids like orcid or doi<br>&nbsp;&nbsp;&nbsp; - structured, machine-readable/actionable format<br>&nbsp;&nbsp;&nbsp; - allows machine validation, verification<br>&nbsp;&nbsp;&nbsp; - allows complex querying by stakeholders, can be integrated in other workflows<br>&nbsp;&nbsp;&nbsp; - dynamic, can be updated through lifecycle<br>"	
dcW42cdSL~	KaTeX and Markdown Basic	expdes summary	4 dynamic data citation recommendations	- recommendations by research data alliance working group on dynamic data citation (rda wg)<br>- 4 recommendations to address 3 challenges<br>- c1) dynamic nature of data (additions/corrections over time)<br>&nbsp;&nbsp;&nbsp; - i) preparing data &amp; query store = data versioning, timestamping, query store → version, timestamp, checksum for each subset<br>- c2) granularity issues (citing precise subsets)<br>&nbsp;&nbsp;&nbsp; - ii) data persistence = query uniqueness, stable sorting, result set verification, query timestamping, query pid, store query, citation text<br>- c3) technology dependencies (stability across system changes)<br>&nbsp;&nbsp;&nbsp; - iii) resolving a pid = landing page (for humans and machines), machine actionability<br>&nbsp;&nbsp;&nbsp; - iv) data infra modifications = technology migration, migration verification → persistent identifier (pid) stays the same even after migration	
swe?>Kbr.B	KaTeX and Markdown Basic	expdes summary	3 types of digital perservation	- physical preservation (bit level) = redundancy, fault tolerance, recovery, storage migration, access privileges<br>- logical preservation = environment emulation for old fileformats, verifiability, reproducibility<br>- semantic preservation = change in context (ie. changing laws, social norms, meaning of concepts)	
"oDo#5Ze97B"	KaTeX and Markdown Basic	expdes summary	7 rules for ethics and privacy by ACM (2017)	- i) awareness = for biases (ie. stereotypes in data mining)<br>- ii) access and redress = for individuals<br>- iii) accountability = taking responsibility (ie. for critical systems)<br>- iv) explanation = of decisions<br>- v) data provenance = data collection, manipulation, biases → overrepresentation of demographics<br>- vi) auditability = of models, data<br>- vii) validation, testing = rigor	
"u#n?b#yXY{"	KaTeX and Markdown Basic	expdes summary	primad reproducibility levels	- some benefits of priming overlap:<br>&nbsp;&nbsp;&nbsp; - any element (except actor, research objective) improves correctness of hypothesis<br>&nbsp;&nbsp;&nbsp; - repeating as an effort achieves determinism through exact replication<br>- primad:<br>&nbsp;&nbsp;&nbsp; - p - platform/stack<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = software env and infrastructure used to conduct experiments<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: porting<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: portability<br>&nbsp;&nbsp;&nbsp; - r - research objective<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = goals and questions being investigated<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: re-using<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: re-purpose, re-using code in different cross-disciplinary settings, resource efficiency, correctness of hypothesis<br>&nbsp;&nbsp;&nbsp; - i - implementation<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = software implementation of methodology<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: re-coding<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: correctness of implementation, portability, efficiency, more outreach<br>&nbsp;&nbsp;&nbsp; - m - method<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = methodology, algorithms<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: validating<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: correctness of hypothesis, validation with different methodogical approaches<br>&nbsp;&nbsp;&nbsp; - a - actors<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = people involved in conducting the experiments<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: independent verification<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: sufficiency of information, independent verification, transparency<br>&nbsp;&nbsp;&nbsp; - d - raw data<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = raw data used in the experiment<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: generalizing<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: re-use for different settings<br>&nbsp;&nbsp;&nbsp; - d - parameters<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - = config values/settings that control the experimental process<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - effort: parameter sweep<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - benefits: robustness testing, sensitivity analysis, parameter sweep, determinism	
u{tqt}=!2F	KaTeX and Markdown Basic	expdes summary	5 documentation frameworks	- prov-o:<br>&nbsp;&nbsp;&nbsp; - w3c recommended ontology for provenance information (change of objects)<br>&nbsp;&nbsp;&nbsp; - models entities, activities, agents<br>&nbsp;&nbsp;&nbsp; - can be integrated with other standards like: foaf, dublin core, premis<br>&nbsp;&nbsp;&nbsp; - versioning, changelog, dependencies, …<br>- huggingface model cards:<br>&nbsp;&nbsp;&nbsp; - readme.md and yaml files for documentation<br>&nbsp;&nbsp;&nbsp; - partially autogenerated<br>&nbsp;&nbsp;&nbsp; - not following fair principles<br>- fair4ml:<br>&nbsp;&nbsp;&nbsp; - ontology for machine learning model metadata<br>&nbsp;&nbsp;&nbsp; - integrates rdf, schema.org, codemeta<br>- codemeta:<br>&nbsp;&nbsp;&nbsp; - for software<br>&nbsp;&nbsp;&nbsp; - handles citations, versions, dependencies, descriptions<br>&nbsp;&nbsp;&nbsp; - standardized metadata templates<br>&nbsp;&nbsp;&nbsp; - cross-platform compatibility with github and package managers<br>- croissant specification:<br>&nbsp;&nbsp;&nbsp; - extension to schema.org for ml dataset description<br>&nbsp;&nbsp;&nbsp; - attempt to standardize documentation<br>&nbsp;&nbsp;&nbsp; - allowing datasets to be loaded without reformatting	
s*gi%_hlym	KaTeX and Markdown Basic	expdes summary	2 types of automated documentation	"- context model:<br>&nbsp;&nbsp;&nbsp; - combines static and dynamic analysis<br>&nbsp;&nbsp;&nbsp; - i) static analysis:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - defines steps, platforms, services, calls, dependencies, licenses<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - uses archimate implemented in OWL<br>&nbsp;&nbsp;&nbsp; - ii) dynamic analysis:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - process migration framework (pmf)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - records system calls and resource access<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - analyzes file formats using pronom and premis<br>- vframework:<br>&nbsp;&nbsp;&nbsp; - record and replay + redeployment + verification<br>&nbsp;&nbsp;&nbsp; - processes expressed as ""research objects""<br>&nbsp;&nbsp;&nbsp; - can trace causes of differing behaviors in reproduced experiments"	
